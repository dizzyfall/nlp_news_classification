{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "950dc538eab99e63",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T11:10:37.367168Z",
     "start_time": "2025-03-20T11:10:33.262989Z"
    },
    "execution": {
     "iopub.execute_input": "2025-03-20T14:00:34.795032Z",
     "iopub.status.busy": "2025-03-20T14:00:34.794898Z",
     "iopub.status.idle": "2025-03-20T14:00:36.108585Z",
     "shell.execute_reply": "2025-03-20T14:00:36.108070Z",
     "shell.execute_reply.started": "2025-03-20T14:00:34.795016Z"
    }
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import os\n",
    "import os.path\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb9133eb632928c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T11:10:37.379710Z",
     "start_time": "2025-03-20T11:10:37.374742Z"
    },
    "execution": {
     "iopub.execute_input": "2025-03-20T14:00:36.114751Z",
     "iopub.status.busy": "2025-03-20T14:00:36.114555Z",
     "iopub.status.idle": "2025-03-20T14:00:36.118807Z",
     "shell.execute_reply": "2025-03-20T14:00:36.118332Z",
     "shell.execute_reply.started": "2025-03-20T14:00:36.114730Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "@Project ：NLPNewsClassification \n",
    "@File    ：data_processing.py\n",
    "@Author  ：DZY\n",
    "@Date    ：2025/3/10 11:55 \n",
    "'''\n",
    "\n",
    "\n",
    "def tokenizer(samples):\n",
    "    \"\"\"\n",
    "    按照单词分词\n",
    "\n",
    "    Args:\n",
    "        samples: 一维列表，每个元素是一个样本\n",
    "\n",
    "    Returns:\n",
    "        二维列表，每个子列表是一个样本，子列表中每个元素是一个词元\n",
    "\n",
    "    Examples:\n",
    "        samples: [\"1 2 3 4\",\"5 6 7 8\"]\n",
    "        Returns: [[1,2,3,4],[5,6,7,8]]\n",
    "\n",
    "    \"\"\"\n",
    "    return [sample.split() for sample in samples]\n",
    "\n",
    "\n",
    "def sample_truncate_pad(sample_tokens, num_steps, padding_token):\n",
    "    \"\"\"\n",
    "    按num_steps长度截取或填充样本\n",
    "\n",
    "    Args:\n",
    "        sample_tokens: 分词后的样本词元列表\n",
    "        num_steps: 样本要截取或填充的长度\n",
    "        padding_token: 填充的词元\n",
    "\n",
    "    Returns:\n",
    "        截取或填充后的样本词元列表\n",
    "    \"\"\"\n",
    "    sample_length = len(sample_tokens)\n",
    "    if sample_length > num_steps:\n",
    "        return sample_tokens[:num_steps]\n",
    "    else:\n",
    "        return sample_tokens + [padding_token] * (num_steps - sample_length)\n",
    "\n",
    "\n",
    "def get_tokens_and_segments(tokens_a, tokens_b=None):\n",
    "    \"\"\"\n",
    "    构造BERT形式的输入序列及片段索引\n",
    "\n",
    "    Args:\n",
    "        tokens_a: 样本序列A,一维列表,其中每一个元素是一个词元\n",
    "        tokens_b: 样本序列B,一维列表,其中每一个元素是一个词元\n",
    "\n",
    "    Returns:\n",
    "        BERT输入形式的序列及片段索引\n",
    "\n",
    "    Examples:\n",
    "        tokens_a: [1, 2, 3]\n",
    "        tokens_b: [4, 5, 6]\n",
    "        Returns:\n",
    "            tokens: ['<cls>', 1, 2, 3, '<sep>', 4, 5, 6, '<sep>']\n",
    "            segments: [0, 0, 0, 0, 0, 1, 1, 1, 1]\n",
    "\n",
    "    \"\"\"\n",
    "    tokens = ['<cls>'] + tokens_a + ['<sep>']\n",
    "    segments = [0] * (len(tokens_a) + 2)\n",
    "    if tokens_b is not None:\n",
    "        tokens += tokens_b + ['<sep>']\n",
    "        segments += [1] * (len(tokens_b) + 1)\n",
    "    return tokens, segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca5ca67c6f6d62e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T11:10:37.390081Z",
     "start_time": "2025-03-20T11:10:37.380713Z"
    },
    "execution": {
     "iopub.execute_input": "2025-03-20T14:00:36.119687Z",
     "iopub.status.busy": "2025-03-20T14:00:36.119445Z",
     "iopub.status.idle": "2025-03-20T14:00:36.127220Z",
     "shell.execute_reply": "2025-03-20T14:00:36.126742Z",
     "shell.execute_reply.started": "2025-03-20T14:00:36.119669Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "@Project ：NLPNewsClassification \n",
    "@File    ：vocabulary.py\n",
    "@Author  ：DZY\n",
    "@Date    ：2025/3/10 17:19 \n",
    "'''\n",
    "\n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):\n",
    "        # Flatten a 2D list if needed\n",
    "        if reserved_tokens is None:\n",
    "            reserved_tokens = []\n",
    "        if tokens is None:\n",
    "            tokens = []\n",
    "        if tokens and isinstance(tokens[0], list):\n",
    "            tokens = [token for line in tokens for token in line]\n",
    "        counter = collections.Counter(tokens)\n",
    "        # print(\"counter:\",list(counter.items())[:10])\n",
    "        self.token_freqs = sorted(counter.items(), key=lambda x: x[1],\n",
    "                                  reverse=True)\n",
    "        # print(\"token_freqs:\",list(self.token_freqs)[:10])\n",
    "        self.idx_to_token = list(\n",
    "            sorted(set(['<unk>'] + reserved_tokens + [token for token, freq in self.token_freqs if freq >= min_freq])))\n",
    "        self.token_to_idx = {token: idx for idx, token in enumerate(self.idx_to_token)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "\n",
    "    def to_tokens(self, indices):\n",
    "        if hasattr(indices, '__len__') and len(indices) > 1:\n",
    "            return [self.idx_to_token[int(index)] for index in indices]\n",
    "        return self.idx_to_token[indices]\n",
    "\n",
    "    def create_vocab_txt(self, vocab_txt_relative_path):\n",
    "        \"\"\"\n",
    "        将词表写到txt文件中\n",
    "\n",
    "        Args:\n",
    "            vocab_txt_relative_path:\n",
    "\n",
    "        Returns:\n",
    "            预训练数据集的词表txt文件\n",
    "        \"\"\"\n",
    "        tokens = []\n",
    "        indices = []\n",
    "        for token, idx in self.token_to_idx.items():\n",
    "            tokens.append(token)\n",
    "            indices.append(idx)\n",
    "        with open(vocab_txt_relative_path, 'w') as file:\n",
    "            for token in tokens:\n",
    "                # 这里词元后面有一个'\\n'，在后续使用词元时要把'\\n'去除掉\n",
    "                file.write(f\"{token}\\n\")\n",
    "\n",
    "    @property\n",
    "    def unk(self):\n",
    "        return self.token_to_idx['<unk>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9526a688456535a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T11:10:37.412179Z",
     "start_time": "2025-03-20T11:10:37.399579Z"
    },
    "execution": {
     "iopub.execute_input": "2025-03-20T14:00:36.135313Z",
     "iopub.status.busy": "2025-03-20T14:00:36.135044Z",
     "iopub.status.idle": "2025-03-20T14:00:36.146169Z",
     "shell.execute_reply": "2025-03-20T14:00:36.145642Z",
     "shell.execute_reply.started": "2025-03-20T14:00:36.135298Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "@Project ：NLPNewsClassification \n",
    "@File    ：attention.py\n",
    "@Author  ：DZY\n",
    "@Date    ：2025/3/11 16:12 \n",
    "'''\n",
    "\n",
    "\n",
    "def masked_softmax(X, valid_lens):\n",
    "    def _sequence_mask(X, valid_len, value=0.0):\n",
    "        maxlen = X.size(1)\n",
    "        mask = torch.arange((maxlen), dtype=torch.float32,\n",
    "                            device=X.device)[None, :] < valid_len[:, None]\n",
    "        X[~mask] = value\n",
    "        return X\n",
    "\n",
    "    if valid_lens is None:\n",
    "        return nn.functional.softmax(X, dim=-1)\n",
    "    else:\n",
    "        shape = X.shape\n",
    "        if valid_lens.dim() == 1:\n",
    "            valid_lens = torch.repeat_interleave(valid_lens, shape[1])\n",
    "        else:\n",
    "            valid_lens = valid_lens.reshape(-1)\n",
    "        # On the last axis, replace masked elements with a very large negative\n",
    "        # value, whose exponentiation outputs 0\n",
    "        X = _sequence_mask(X.reshape(-1, shape[-1]), valid_lens, value=-1e6)\n",
    "        return nn.functional.softmax(X.reshape(shape), dim=-1)\n",
    "\n",
    "\n",
    "class DotProductAttention(nn.Module):\n",
    "    def __init__(self, dropout):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, queries, keys, values, valid_lens=None):\n",
    "        d = queries.shape[-1]\n",
    "        attention_scores = torch.bmm(queries, keys.transpose(1, 2)) / math.sqrt(d)\n",
    "        self.attention_weights = masked_softmax(attention_scores, valid_lens)\n",
    "        return torch.bmm(self.dropout(self.attention_weights), values)\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, query_size, key_size, value_size, num_hiddens, num_heads, dropout, use_bias=False, **kwargs):\n",
    "        super(MultiHeadAttention, self).__init__(**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = DotProductAttention(dropout)\n",
    "        self.W_q = nn.Linear(query_size, num_hiddens, bias=use_bias)\n",
    "        self.W_k = nn.Linear(key_size, num_hiddens, bias=use_bias)\n",
    "        self.W_v = nn.Linear(value_size, num_hiddens, bias=use_bias)\n",
    "        self.W_o = nn.Linear(num_hiddens, num_hiddens, bias=use_bias)\n",
    "\n",
    "    def forward(self, queries, keys, values, valid_lens):\n",
    "        queries = self.transpose_qkv(self.W_q(queries))\n",
    "        keys = self.transpose_qkv(self.W_k(keys))\n",
    "        values = self.transpose_qkv(self.W_v(values))\n",
    "        # print(\"queries.shape:\",queries.shape,\"keys.shape:\",keys.shape,\"values.shape:\",values.shape)\n",
    "\n",
    "        if valid_lens is not None:\n",
    "            valid_lens = torch.repeat_interleave(valid_lens, repeats=self.num_heads, dim=0)\n",
    "\n",
    "        output = self.attention(queries, keys, values, valid_lens)\n",
    "        # print(\"output.shape:\",output.shape)\n",
    "\n",
    "        output_concat = self.transpose_output(output)\n",
    "        # print(\"output_concat.shape:\",output_concat.shape)\n",
    "\n",
    "        result = self.W_o(output_concat)\n",
    "        # print(\"result.shape:\", result.shape)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def transpose_qkv(self, X):\n",
    "        # (batch_size,num_steps,num_heads,num_hiddens)->(batch_size,num_steps,num_heads,num_hiddens/num_heads)\n",
    "        X = X.reshape(X.shape[0], X.shape[1], self.num_heads, -1)\n",
    "        # ->(batch_size,num_heads,num_steps,num_hiddens/num_heads)\n",
    "        X = X.permute(0, 2, 1, 3)\n",
    "        # ->(batch_size*num_heads,num_steps,num_hiddens/num_heads)\n",
    "        return X.reshape(-1, X.shape[2], X.shape[3])\n",
    "\n",
    "    def transpose_output(self, X):\n",
    "        # (batch_size*num_heads,num_steps,num_hiddens/num_heads)->(batch_size,num_heads,num_steps,num_hiddens/num_heads)\n",
    "        X = X.reshape(-1, self.num_heads, X.shape[1], X.shape[2])\n",
    "        # ->(batch_size,num_steps,num_heads,num_hiddens/num_heads)\n",
    "        X = X.permute(0, 2, 1, 3)\n",
    "        # ->(batch_size,num_steps,num_hiddens)\n",
    "        return X.reshape(X.shape[0], X.shape[1], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd638981af8539c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T11:10:37.441095Z",
     "start_time": "2025-03-20T11:10:37.432143Z"
    },
    "execution": {
     "iopub.execute_input": "2025-03-20T14:00:36.161570Z",
     "iopub.status.busy": "2025-03-20T14:00:36.161316Z",
     "iopub.status.idle": "2025-03-20T14:00:36.169123Z",
     "shell.execute_reply": "2025-03-20T14:00:36.168689Z",
     "shell.execute_reply.started": "2025-03-20T14:00:36.161546Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "@Project ：NLPNewsClassification \n",
    "@File    ：bert.py\n",
    "@Author  ：DZY\n",
    "@Date    ：2025/3/11 21:32 \n",
    "'''\n",
    "\n",
    "\n",
    "class BERTEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, query_size, key_size, value_size, num_hiddens, normalized_shape, ffn_num_input,\n",
    "                 ffn_num_hiddens, num_heads, num_layers, dropout, max_len=1000, **kwargs):\n",
    "        super(BERTEncoder, self).__init__(**kwargs)\n",
    "        self.token_embedding = nn.Embedding(vocab_size, num_hiddens)\n",
    "        self.segment_embedding = nn.Embedding(2, num_hiddens)\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, max_len, num_hiddens))\n",
    "        self.blocks = nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            self.blocks.add_module(f\"TransformerEncoderBlock:{i}\",\n",
    "                                   TransformerEncoderBlock(query_size, key_size, value_size, num_hiddens,\n",
    "                                                           normalized_shape, ffn_num_input, ffn_num_hiddens,\n",
    "                                                           num_heads, dropout, True))\n",
    "\n",
    "    def forward(self, tokens, segments, valid_lens):\n",
    "        X = self.token_embedding(tokens) + self.segment_embedding(segments)\n",
    "        X = X + self.pos_embedding.data[:, :X.shape[1], :]\n",
    "        for block in self.blocks:\n",
    "            X = block(X, valid_lens)\n",
    "        return X\n",
    "\n",
    "\n",
    "class BERTLM(nn.Module):\n",
    "    def __init__(self, vocab_size, query_size, key_size, value_size, num_hiddens, normalized_shape, ffn_num_input,\n",
    "                 ffn_num_hiddens, num_heads, num_layers, mlm_in_features, mlm_hiddens, nsp_in_features, nsp_hiddens,\n",
    "                 dropout, max_len=1000, **kwargs):\n",
    "        super(BERTLM, self).__init__(**kwargs)\n",
    "        self.encoder = BERTEncoder(vocab_size, query_size, key_size, value_size, num_hiddens, normalized_shape,\n",
    "                                   ffn_num_input, ffn_num_hiddens, num_heads, num_layers, dropout, max_len=max_len)\n",
    "        self.mlm = MaskLM(mlm_in_features, mlm_hiddens, vocab_size)\n",
    "        self.nsp = NextSentencePred(nsp_in_features, nsp_hiddens)\n",
    "\n",
    "    def forward(self, tokens, segments, valid_lens=None, pred_position=None):\n",
    "        encoded_X = self.encoder(tokens, segments, valid_lens)\n",
    "        if pred_position is not None:\n",
    "            mlm_Y_hat = self.mlm(encoded_X, pred_position)\n",
    "        else:\n",
    "            mlm_Y_hat = None\n",
    "        # 下一句子预测只需要<cls>这个特殊标识符的信息\n",
    "        nsp_Y_hat = self.nsp(encoded_X[:, 0, :])\n",
    "        return encoded_X, mlm_Y_hat, nsp_Y_hat"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 7,
   "source": [
    "'''\n",
    "@Project ：NLPNewsClassification \n",
    "@File    ：encoder.py\n",
    "@Author  ：DZY\n",
    "@Date    ：2025/3/11 16:13 \n",
    "'''\n",
    "\n",
    "\n",
    "class PositionWiseFFN(nn.Module):\n",
    "    def __init__(self, ffn_num_input, ffn_num_hiddens, ffn_num_outputs, **kwargs):\n",
    "        super(PositionWiseFFN, self).__init__(**kwargs)\n",
    "        self.dense1 = nn.Linear(ffn_num_input, ffn_num_hiddens)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dense2 = nn.Linear(ffn_num_hiddens, ffn_num_outputs)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.dense2(self.relu(self.dense1(X)))\n",
    "\n",
    "\n",
    "class AddNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, dropout, **kwargs):\n",
    "        super(AddNorm, self).__init__(**kwargs)\n",
    "        self.layernorm = nn.LayerNorm(normalized_shape)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, X, Y):\n",
    "        return self.layernorm(self.dropout(Y) + X)\n",
    "\n",
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, query_size, key_size, value_size, num_hiddens, normalized_shape, ffn_num_input, ffn_num_hiddens,\n",
    "                 num_heads, dropout, use_bias=False, **kwargs):\n",
    "        super(TransformerEncoderBlock, self).__init__(**kwargs)\n",
    "        self.multi_head_attention = MultiHeadAttention(query_size, key_size, value_size, num_hiddens, num_heads,\n",
    "                                                       dropout, use_bias)\n",
    "        self.add_norm1 = AddNorm(normalized_shape, dropout)\n",
    "        self.ffn = PositionWiseFFN(ffn_num_input, ffn_num_hiddens, num_hiddens)\n",
    "        self.add_norm2 = AddNorm(normalized_shape, dropout)\n",
    "\n",
    "    def forward(self, X, valid_lens):\n",
    "        Y = self.add_norm1(X, self.multi_head_attention(X, X, X, valid_lens))\n",
    "        return self.add_norm2(Y, self.ffn(Y))"
   ],
   "id": "2b51553fe0fdccb4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 8,
   "source": [
    "'''\n",
    "@Project ：NLPNewsClassification \n",
    "@File    ：pretrain_tasks.py\n",
    "@Author  ：DZY\n",
    "@Date    ：2025/3/12 11:34 \n",
    "'''\n",
    "\n",
    "\n",
    "class MaskLM(nn.Module):\n",
    "    def __init__(self, mlm_in_features, mlm_hiddens, vocab_size, **kwargs):\n",
    "        super(MaskLM, self).__init__(**kwargs)\n",
    "        self.mlp = nn.Sequential(nn.Linear(mlm_in_features, mlm_hiddens), nn.ReLU(), nn.LayerNorm(mlm_hiddens),\n",
    "                                 nn.Linear(mlm_hiddens, vocab_size))\n",
    "\n",
    "    def forward(self, X, pred_position):\n",
    "        # pred_position:(batch_size,num_pred_position)\n",
    "        # X:(batch_size,num_steps,num_hiddens)\n",
    "        num_pred_position = pred_position.shape[-1]\n",
    "        batch_size = X.shape[0]\n",
    "        # pred_position:(batch_size*num_pred_position)\n",
    "        pred_position = pred_position.reshape(-1)\n",
    "        batch_indices = torch.arange(0, batch_size)\n",
    "        batch_indices = torch.repeat_interleave(batch_indices, num_pred_position)\n",
    "        # 花式索引\n",
    "        # masked_X:(batch_size*num_pred_position,num_hiddens)\n",
    "        # 取的是每个掩蔽位置的词向量\n",
    "        masked_X = X[batch_indices, pred_position]\n",
    "        # masked_X:(batch_size,num_pred_position,num_hiddens)\n",
    "        masked_X = masked_X.reshape((batch_size, num_pred_position, -1))\n",
    "\n",
    "        mlm_Y_hat = self.mlp(masked_X)\n",
    "        return mlm_Y_hat\n",
    "\n",
    "\n",
    "class NextSentencePred(nn.Module):\n",
    "    def __init__(self, nsp_in_features, nsp_hiddens, **kwargs):\n",
    "        super(NextSentencePred, self).__init__(**kwargs)\n",
    "        self.mlp = nn.Sequential(nn.Linear(nsp_in_features, nsp_hiddens), nn.Tanh(), nn.Linear(nsp_hiddens, 2))\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.mlp(X)\n"
   ],
   "id": "a5ff2e0eb5001837"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "'''\n",
    "@Project ：NLPNewsClassification \n",
    "@File    ：checkpoints.py\n",
    "@Author  ：DZY\n",
    "@Date    ：2025/3/24 19:07 \n",
    "'''\n",
    "\n",
    "\n",
    "def save_pretrain_checkpoint(model, optimizer, step, checkpoint_pretrain_info_tuple, checkpoints_relative_path,\n",
    "                             checkpoint_dir_name,\n",
    "                             checkpoint_file_name):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        model:\n",
    "        optimizer:\n",
    "        step:\n",
    "        checkpoint_pretrain_info_tuple:\n",
    "        checkpoints_relative_path:\n",
    "        checkpoint_dir_name:\n",
    "        checkpoint_file_name:\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    script_directory = os.getcwd()\n",
    "    checkpoint_dir_path = os.path.join(script_directory, checkpoints_relative_path, checkpoint_dir_name)\n",
    "    if not os.path.exists(checkpoint_dir_path):\n",
    "        os.makedirs(checkpoint_dir_path)\n",
    "    # 拼接检查点路径\n",
    "    checkpoint_file_path = os.path.join(checkpoint_dir_path, checkpoint_file_name)\n",
    "    torch.save({\n",
    "        'step': step,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'total_mlm_loss': checkpoint_pretrain_info_tuple[0],\n",
    "        'total_processed_samples': checkpoint_pretrain_info_tuple[1],\n",
    "        'cum_time_list': checkpoint_pretrain_info_tuple[2]\n",
    "    }, checkpoint_file_path)\n",
    "    print(f\"检查点已保存到 {checkpoint_file_path}\")\n",
    "\n",
    "\n",
    "def save_finetuning_model(model, checkpoints_relative_path,\n",
    "                          checkpoint_dir_name,\n",
    "                          checkpoint_file_name):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        model:\n",
    "        checkpoints_relative_path:\n",
    "        checkpoint_dir_name:\n",
    "        checkpoint_file_name:\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    script_directory = os.getcwd()\n",
    "    checkpoint_dir_path = os.path.join(script_directory, checkpoints_relative_path, checkpoint_dir_name)\n",
    "    if not os.path.exists(checkpoint_dir_path):\n",
    "        os.makedirs(checkpoint_dir_path)\n",
    "    # 拼接检查点路径\n",
    "    checkpoint_file_path = os.path.join(checkpoint_dir_path, checkpoint_file_name)\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "    }, checkpoint_file_path)\n",
    "    print(f\"模型参数已保存到 {checkpoint_file_path}\")\n",
    "\n",
    "\n",
    "def load_checkpoint(model, checkpoint_relative_path, device, optimizer=None, scheduler=None):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        model:\n",
    "        checkpoint_relative_path:\n",
    "        device:\n",
    "        optimizer:\n",
    "        scheduler:\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    # 加载检查点，并加载模型参数\n",
    "    checkpoint = load_pretrained_model_params(model, checkpoint_relative_path, device)\n",
    "    if optimizer:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        print(\"加载预训练BERT优化器参数成功\")\n",
    "    if scheduler and checkpoint['scheduler_state_dict']:\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        print(\"加载预训练BERT调度器参数成功\")\n",
    "\n",
    "    # 恢复训练步数\n",
    "    step = checkpoint['step']\n",
    "    total_mlm_loss = checkpoint['total_mlm_loss'],\n",
    "    total_processed_samples = checkpoint['total_processed_samples']\n",
    "    cum_time_list = checkpoint['cum_time_list']\n",
    "    print(f\"恢复步数: {step} \"\n",
    "          f\"恢复当前总损失: {total_mlm_loss} \"\n",
    "          f\"恢复当前已处理样本数: {total_processed_samples} \"\n",
    "          f\"恢复当前训练迭代时间列表: {cum_time_list}\")\n",
    "\n",
    "    return step, total_mlm_loss, total_processed_samples, cum_time_list\n",
    "\n",
    "\n",
    "def _process_model_state_dict_keys_name(model_state_dict):\n",
    "    return {key.replace(\"module.\", \"\"): value for key, value in model_state_dict.items()}\n",
    "\n",
    "\n",
    "def load_pretrained_model_params(model, checkpoint_relative_path, device):\n",
    "    script_directory = os.getcwd()\n",
    "    checkpoint_path = os.path.join(script_directory, checkpoint_relative_path)\n",
    "    # 加载检查点\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    print(f\"加载检查点路径{checkpoint_relative_path}\")\n",
    "\n",
    "    checkpoint['model_state_dict'] = _process_model_state_dict_keys_name(checkpoint['model_state_dict'])\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(\"加载预训练BERT模型参数成功\")\n",
    "    return checkpoint\n",
    "\n",
    "\n",
    "def save_pretrain_info(pretrain_results_relative_path, pretrain_result):\n",
    "    script_directory = os.getcwd()\n",
    "    pretrain_results_path = os.path.join(script_directory, pretrain_results_relative_path)\n",
    "    with open(pretrain_results_path, 'w') as file:\n",
    "        file.write(pretrain_result)"
   ],
   "id": "9b8cd851e025d5c5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 10,
   "source": [
    "'''\n",
    "@Project ：NLPNewsClassification \n",
    "@File    ：environment.py\n",
    "@Author  ：DZY\n",
    "@Date    ：2025/3/14 15:41 \n",
    "'''\n",
    "\n",
    "\n",
    "def use_cpu():\n",
    "    \"\"\"Get the CPU device.\n",
    "\n",
    "    Defined in :numref:`sec_use_gpu`\"\"\"\n",
    "    return torch.device('cpu')\n",
    "\n",
    "\n",
    "def use_gpu(i=0):\n",
    "    \"\"\"Get a GPU device.\n",
    "\n",
    "    Defined in :numref:`sec_use_gpu`\"\"\"\n",
    "    return torch.device(f'cuda:{i}')\n",
    "\n",
    "\n",
    "def num_gpus():\n",
    "    \"\"\"Get the number of available GPUs.\n",
    "\n",
    "    Defined in :numref:`sec_use_gpu`\"\"\"\n",
    "    return torch.cuda.device_count()\n",
    "\n",
    "\n",
    "def try_gpu(i=0):\n",
    "    \"\"\"Return gpu(i) if exists, otherwise return cpu().\n",
    "\n",
    "    Defined in :numref:`sec_use_gpu`\"\"\"\n",
    "    if num_gpus() >= i + 1:\n",
    "        return use_gpu(i)\n",
    "    return use_cpu()\n",
    "\n",
    "\n",
    "def try_all_gpus():\n",
    "    \"\"\"Return all available GPUs, or [cpu(),] if no GPU exists.\n",
    "\n",
    "    Defined in :numref:`sec_use_gpu`\"\"\"\n",
    "    return [use_gpu(i) for i in range(num_gpus())]\n"
   ],
   "id": "956f5006950be184"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "'''\n",
    "@Project ：NLPNewsClassification \n",
    "@File    ：scheduler.py\n",
    "@Author  ：DZY\n",
    "@Date    ：2025/3/25 11:35 \n",
    "'''\n",
    "\n",
    "\n",
    "class BERTScheduler():\n",
    "    def __init__(self, optimizer, num_hiddens, warmup_steps, current_step=0):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            optimizer: 优化器对象\n",
    "            num_hiddens: 词向量长度\n",
    "            warmup_steps: warmup步数\n",
    "            current_step: 当前epoch（用于恢复训练）\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "        self.optimizer = optimizer\n",
    "        self.init_lr = np.power(num_hiddens, -0.5)\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.current_step = current_step\n",
    "        self.step()\n",
    "\n",
    "    def get_lr(self):\n",
    "        if self.current_step < self.warmup_steps:\n",
    "            # warmup阶段，线性增加学习率\n",
    "            return self.init_lr * np.power(self.warmup_steps, -1.5) * self.current_step\n",
    "        else:\n",
    "            # 线性衰退\n",
    "            return self.init_lr * np.power(self.current_step, -0.5)\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"更新学习率\"\"\"\n",
    "        self.current_step += 1\n",
    "        lr = self.get_lr()\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr"
   ],
   "id": "bc85d48bf459f298"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 11,
   "source": [
    "'''\n",
    "@Project ：NLPNewsClassification \n",
    "@File    ：timer.py\n",
    "@Author  ：DZY\n",
    "@Date    ：2025/3/14 17:05 \n",
    "'''\n",
    "\n",
    "\n",
    "class Timer():\n",
    "    def __init__(self):\n",
    "        self.iter_step_time_list = []\n",
    "\n",
    "    def start(self):\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def stop(self):\n",
    "        self.stop_time = time.time()\n",
    "        self.iter_step_time_list.append(self.stop_time - self.start_time)\n",
    "        return self.iter_step_time_list[-1]\n",
    "\n",
    "    def get_time_diff(self):\n",
    "        return self.iter_step_time_list[-1]\n",
    "\n",
    "    def get_iter_step_time_list(self):\n",
    "        return self.iter_step_time_list\n",
    "\n",
    "    def get_step_time_diff(self, start_step_index, end_step_index):\n",
    "        return sum(self.iter_step_time_list[start_step_index:end_step_index])\n",
    "\n",
    "    def get_total_time(self):\n",
    "        return sum(self.iter_step_time_list)\n",
    "\n",
    "    def get_avg_time(self):\n",
    "        return self.get_total_time() / len(self.iter_step_time_list)\n",
    "\n",
    "    def get_cumulate_time(self):\n",
    "        \"\"\"\n",
    "\n",
    "        Returns:\n",
    "            返回累积时间\n",
    "\n",
    "        \"\"\"\n",
    "        return np.array(self.iter_step_time_list).cumsum().tolist()\n",
    "\n",
    "\n",
    "# 将训练时长格式化为 小时:分:秒\n",
    "def format_duration(seconds):\n",
    "    # 计算小时、分钟和秒\n",
    "    hours = int(seconds // 3600)\n",
    "    minutes = int((seconds % 3600) // 60)\n",
    "    seconds = int(seconds % 60)\n",
    "    # 格式化为 hh:mm:ss\n",
    "    return f\"{hours:02d}:{minutes:02d}:{seconds:02d}\"\n"
   ],
   "id": "878c3ca07ca3ba40"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6595a416a23f506",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T11:10:37.474188Z",
     "start_time": "2025-03-20T11:10:37.456178Z"
    },
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-03-20T14:00:36.181364Z",
     "iopub.status.busy": "2025-03-20T14:00:36.181095Z",
     "iopub.status.idle": "2025-03-20T14:00:36.197455Z",
     "shell.execute_reply": "2025-03-20T14:00:36.197045Z",
     "shell.execute_reply.started": "2025-03-20T14:00:36.181343Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "@Project ：NLPNewsClassification \n",
    "@File    ：pretrain_data_create.py\n",
    "@Author  ：DZY\n",
    "@Date    ：2025/3/12 16:25 \n",
    "'''\n",
    "\n",
    "\n",
    "def _get_inputs(samples_tokens, max_len):\n",
    "    \"\"\"\n",
    "    构造所有样本的BERT形式的输入序列\n",
    "\n",
    "    Args:\n",
    "        samples_tokens: 所有样本序列，二维列表，每个子列表是一个样本序列，子列表每个元素是一个词元\n",
    "        max_len: BERT输入序列最大长度\n",
    "\n",
    "    Returns:\n",
    "        所有样本的BERT形式的输入序列及其片段索引\n",
    "        元组列表：[(样本的BERT形式的输入序列,其片段索引),...]\n",
    "\n",
    "    \"\"\"\n",
    "    inputs = []\n",
    "    for sample_tokens in samples_tokens:\n",
    "        # 如果原本输入序列加上<cls>'和'<sep>是否会超过max_len\n",
    "        if (len(sample_tokens) + 2) > max_len:\n",
    "            # 超过就丢掉\n",
    "            continue\n",
    "        tokens, segments = get_tokens_and_segments(sample_tokens)\n",
    "        inputs.append((tokens, segments))\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def _replace_mlm_tokens(tokens, candidate_pred_positions, num_mlm_preds, vocab):\n",
    "    \"\"\"\n",
    "    用于掩蔽语言模型替换词元\n",
    "\n",
    "    Args:\n",
    "        tokens: 用于MLM的输入序列（BERT形式的输入），一维列表，每个元素是一个词元，包含特殊标识符\n",
    "        candidate_pred_positions: 候选需要替换（预测）的词元位置下标索引（不包括特殊标识符，特殊标识符不被预测）\n",
    "        num_mlm_preds: 需要替换（预测）的词元数量\n",
    "        vocab: 词表\n",
    "\n",
    "    Returns:\n",
    "        随机替换后的序列\n",
    "        替换的位置和替换之前的词元组成的元组列表\n",
    "\n",
    "    \"\"\"\n",
    "    # 复制一份MLM输入序列词元列表\n",
    "    mlm_input_tokens = [token for token in tokens]\n",
    "    # 预测的词元位置索引和被替换前的词元\n",
    "    pred_positions_and_labels = []\n",
    "    # 随机打乱候选替换位置索引\n",
    "    random.shuffle(candidate_pred_positions)\n",
    "    for mlm_pred_position in candidate_pred_positions:\n",
    "        if len(pred_positions_and_labels) > num_mlm_preds:\n",
    "            break\n",
    "        masked_tokens = None\n",
    "        # 80%时间替换为<mask>\n",
    "        if random.random() < 0.8:\n",
    "            masked_token = '<mask>'\n",
    "        else:\n",
    "            # 10%时间保持不变\n",
    "            if random.random() < 0.5:\n",
    "                masked_token = tokens[mlm_pred_position]\n",
    "            # 10%时间替换为随机词元\n",
    "            else:\n",
    "                masked_token = random.choice(vocab.idx_to_token)\n",
    "        # 替换输入序列中指定位置的词元\n",
    "        mlm_input_tokens[mlm_pred_position] = masked_token\n",
    "        # 保存替换的位置和替换之前的词元\n",
    "        pred_positions_and_labels.append((mlm_pred_position, tokens[mlm_pred_position]))\n",
    "    return mlm_input_tokens, pred_positions_and_labels\n",
    "\n",
    "\n",
    "def _get_mlm_data(tokens, vocab):\n",
    "    \"\"\"\n",
    "    生成用于掩蔽语言模型的数据\n",
    "\n",
    "    Args:\n",
    "        tokens: 用于MLM的输入序列（BERT形式的输入），一维列表，每个元素是一个词元，包含特殊标识符\n",
    "        vocab: 词表\n",
    "\n",
    "    Returns:\n",
    "        替换后的序列中所有词元的索引下标列表\n",
    "        预测位置的下标列表\n",
    "        替换前的词元索引下标列表\n",
    "\n",
    "    \"\"\"\n",
    "    # 候选预测位置索引\n",
    "    candidate_pred_position = []\n",
    "    for index, token in enumerate(tokens):\n",
    "        # 特殊标识符不参与词元预测\n",
    "        if token in ['<cls>', '<sep>']:\n",
    "            continue\n",
    "        candidate_pred_position.append(index)\n",
    "    # MLM中只预测15%的词元\n",
    "    num_mlm_preds = max(1, round(len(tokens) * 0.15))\n",
    "    mlm_input_tokens, pred_positions_and_labels = _replace_mlm_tokens(tokens, candidate_pred_position, num_mlm_preds,\n",
    "                                                                      vocab)\n",
    "    pred_positions_and_labels = sorted(pred_positions_and_labels, key=lambda x: x[0])\n",
    "    pred_positions = [position_and_label_tuple[0] for position_and_label_tuple in pred_positions_and_labels]\n",
    "    pred_labels = [position_and_label_tuple[1] for position_and_label_tuple in pred_positions_and_labels]\n",
    "    return vocab[mlm_input_tokens], pred_positions, vocab[pred_labels]\n",
    "\n",
    "\n",
    "def _pad_bert_inputs(examples, max_len, vocab):\n",
    "    max_num_mlm_preds = round(max_len * 0.15)\n",
    "    all_token_ids, all_segments, valid_lens, = [], [], []\n",
    "    all_pred_positions, all_mlm_weights, all_mlm_labels = [], [], []\n",
    "    for (token_ids, pred_positions, mlm_pred_label_ids, segments) in examples:\n",
    "        all_token_ids.append(torch.tensor(token_ids + [vocab['<pad>']] * (\n",
    "                max_len - len(token_ids)), dtype=torch.long))\n",
    "        all_segments.append(torch.tensor(segments + [0] * (\n",
    "                max_len - len(segments)), dtype=torch.long))\n",
    "        # valid_lens不包括'<pad>'的计数\n",
    "        valid_lens.append(torch.tensor(len(token_ids), dtype=torch.float32))\n",
    "        all_pred_positions.append(torch.tensor(pred_positions + [0] * (\n",
    "                max_num_mlm_preds - len(pred_positions)), dtype=torch.long))\n",
    "        # 填充词元的预测将通过乘以0权重在损失中过滤掉\n",
    "        all_mlm_weights.append(\n",
    "            torch.tensor([1.0] * len(mlm_pred_label_ids) + [0.0] * (\n",
    "                    max_num_mlm_preds - len(pred_positions)),\n",
    "                         dtype=torch.float32))\n",
    "        all_mlm_labels.append(torch.tensor(mlm_pred_label_ids + [0] * (\n",
    "                max_num_mlm_preds - len(mlm_pred_label_ids)), dtype=torch.long))\n",
    "    return (all_token_ids, all_segments, valid_lens, all_pred_positions,\n",
    "            all_mlm_weights, all_mlm_labels)\n",
    "\n",
    "\n",
    "class PretrainDataset(Dataset):\n",
    "    def __init__(self, samples, max_len):\n",
    "        # 原始样本序列按单词分词\n",
    "        samples_tokens = tokenizer(samples)\n",
    "        self.vocab = Vocab(samples_tokens, min_freq=5, reserved_tokens=['<cls>', '<sep>', '<pad>', '<mask>'])\n",
    "        # 获取所有BERT形式的输入序列\n",
    "        examples = []\n",
    "        examples.extend(_get_inputs(samples_tokens, max_len))\n",
    "        examples = [(_get_mlm_data(tokens, self.vocab) + (segments,)) for tokens, segments in examples]\n",
    "        (self.all_token_ids, self.all_segments, self.valid_lens,\n",
    "         self.all_pred_positions, self.all_mlm_weights,\n",
    "         self.all_mlm_labels) = _pad_bert_inputs(examples, max_len, self.vocab)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.all_token_ids[idx], self.all_segments[idx],\n",
    "                self.valid_lens[idx], self.all_pred_positions[idx],\n",
    "                self.all_mlm_weights[idx], self.all_mlm_labels[idx])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_token_ids)\n",
    "\n",
    "\n",
    "def get_dataloader_workers(num_workers):\n",
    "    \"\"\"\n",
    "    使用指定个数进程来读取数据\n",
    "\n",
    "    Args:\n",
    "        num_workers: 进程数\n",
    "\n",
    "    Returns:\n",
    "        进程数\n",
    "\n",
    "    \"\"\"\n",
    "    return num_workers\n",
    "\n",
    "\n",
    "def read_pretrain_data(pretrain_sentences_relative_path):\n",
    "    script_directory = os.getcwd()\n",
    "    full_path = os.path.join(script_directory, pretrain_sentences_relative_path)\n",
    "    with open(full_path, 'r') as file:\n",
    "        samples = file.readlines()\n",
    "    return samples\n",
    "\n",
    "\n",
    "def load_pretrain_data(pretrain_data_relative_path, vocab_txt_relative_path, batch_size, max_len):\n",
    "    num_workers = get_dataloader_workers(0)\n",
    "    samples = read_pretrain_data(pretrain_data_relative_path)\n",
    "    pretrain_set = PretrainDataset(samples, max_len)\n",
    "    pretrain_set.vocab.create_vocab_txt(vocab_txt_relative_path)\n",
    "    pretrain_iter = torch.utils.data.DataLoader(pretrain_set, batch_size, shuffle=True, num_workers=num_workers)\n",
    "    return pretrain_iter, pretrain_set.vocab\n",
    "\n",
    "# batch_size, max_len = 128, 256\n",
    "# train_iter, vocab = load_pretrain_data(batch_size, max_len)\n",
    "#\n",
    "# for (tokens_X, segments_X, valid_lens_X, pred_positions_X, mlm_weights_X,mlm_Y) in train_iter:\n",
    "#     print(tokens_X.shape, segments_X.shape, valid_lens_X.shape,\n",
    "#           pred_positions_X.shape, mlm_weights_X.shape, mlm_Y.shape)\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49a966373f02bf2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T11:10:37.490759Z",
     "start_time": "2025-03-20T11:10:37.475191Z"
    },
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-03-20T14:00:36.198313Z",
     "iopub.status.busy": "2025-03-20T14:00:36.198030Z",
     "iopub.status.idle": "2025-03-20T14:00:36.211771Z",
     "shell.execute_reply": "2025-03-20T14:00:36.211379Z",
     "shell.execute_reply.started": "2025-03-20T14:00:36.198292Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "@Project ：NLPNewsClassification\n",
    "@File    ：pretrain_bert.py\n",
    "@Author  ：DZY\n",
    "@Date    ：2025/3/14 12:38\n",
    "'''\n",
    "\n",
    "\n",
    "def load_pretrain_checkpoint(model, optimizer, device, checkpoint_relative_path=None):\n",
    "    if checkpoint_relative_path is None:\n",
    "        step = 0\n",
    "        total_mlm_loss = 0.0\n",
    "        total_processed_samples = 0.0\n",
    "        cum_time_list = []\n",
    "        return step, total_mlm_loss, total_processed_samples, cum_time_list\n",
    "    step, total_mlm_loss, total_processed_samples, cum_time_list = load_checkpoint(model,\n",
    "                                                                                   checkpoint_relative_path,\n",
    "                                                                                   device, optimizer)\n",
    "    return step, total_mlm_loss, total_processed_samples, cum_time_list\n",
    "\n",
    "\n",
    "class PretrainBERT():\n",
    "    def __init__(self, current_step=0, total_mlm_loss=0.0, total_processed_samples=0.0, cum_time_list=None):\n",
    "        if cum_time_list is None:\n",
    "            cum_time_list = []\n",
    "        self.current_step = current_step\n",
    "        self.total_mlm_loss = total_mlm_loss\n",
    "        self.total_processed_samples = total_processed_samples\n",
    "        self.cum_time_list = cum_time_list\n",
    "\n",
    "    def _get_bert_batch_loss(self, net, loss, vocab_size, tokens_X, segments_X, valid_lens_X, pred_positions_X,\n",
    "                             mlm_weights_X, mlm_Y):\n",
    "        _, mlm_Y_hat, _ = net(tokens_X, segments_X, valid_lens_X, pred_positions_X)\n",
    "        mlm_loss = loss(mlm_Y_hat.reshape(-1, vocab_size), mlm_Y.reshape(-1)) * mlm_weights_X.reshape(-1, 1)\n",
    "        mlm_loss = mlm_loss.sum() / (mlm_weights_X.sum() + 1e-8)\n",
    "        return mlm_loss\n",
    "\n",
    "    def pretrain(self, pretrain_iter, net, loss, vocab_size, pretrain_optimizer, pretrain_scheduler,\n",
    "                 num_pretrain_iter_steps,\n",
    "                 checkpoints_relative_path, devices):\n",
    "        net = nn.DataParallel(net, device_ids=devices).to(devices[0])\n",
    "        pretrain_timer = Timer()\n",
    "        num_pretrain_iter_steps_reached = False\n",
    "        save_checkpoint_interval = 1000\n",
    "        pretrain_info_interval = 100\n",
    "\n",
    "        print(\"start pretraining...\")\n",
    "        while self.current_step < num_pretrain_iter_steps and not num_pretrain_iter_steps_reached:\n",
    "            for tokens_X, segments_X, valid_lens_X, pred_positions_X, mlm_weights_X, mlm_Y in pretrain_iter:\n",
    "                tokens_X = tokens_X.to(devices[0])\n",
    "                segments_X = segments_X.to(devices[0])\n",
    "                valid_lens_X = valid_lens_X.to(devices[0])\n",
    "                pred_positions_X = pred_positions_X.to(devices[0])\n",
    "                mlm_weights_X = mlm_weights_X.to(devices[0])\n",
    "                mlm_Y = mlm_Y.to(devices[0])\n",
    "\n",
    "                pretrain_timer.start()\n",
    "                pretrain_optimizer.zero_grad()\n",
    "                mlm_loss = self._get_bert_batch_loss(net, loss, vocab_size, tokens_X, segments_X, valid_lens_X,\n",
    "                                                     pred_positions_X, mlm_weights_X, mlm_Y)\n",
    "                mlm_loss.backward()\n",
    "                pretrain_optimizer.step()\n",
    "\n",
    "                if pretrain_scheduler is not None:\n",
    "                    pretrain_scheduler.step()\n",
    "                pretrain_timer.stop()\n",
    "\n",
    "                self.total_mlm_loss += mlm_loss.item()\n",
    "                self.total_processed_samples += tokens_X.shape[0]\n",
    "\n",
    "                if (self.current_step + 1) % pretrain_info_interval == 0:\n",
    "                    self.cum_time_list = pretrain_timer.get_cumulate_time()\n",
    "                    print(\n",
    "                        f\"Iter Steps: {self.current_step + 2 - pretrain_info_interval}-{self.current_step + 1} ---- \"\n",
    "                        f\"Avg MLM Loss: {self.total_mlm_loss / (self.current_step + 1):.4f} ---- \"\n",
    "                        f\"Cumulative Iter Time: {self.cum_time_list[self.current_step]:.4f} sec\")\n",
    "\n",
    "                if (self.current_step + 1) % save_checkpoint_interval == 0:\n",
    "                    checkpoint_dir_name = f\"checkpoint_step_{self.current_step + 1}\"\n",
    "                    checkpoint_file_name = f\"checkpoint_step_{self.current_step + 1}.pth\"\n",
    "                    checkpoint_pretrain_info_tuple = (\n",
    "                        self.total_mlm_loss, self.total_processed_samples, self.cum_time_list)\n",
    "                    save_pretrain_checkpoint(net, pretrain_optimizer,\n",
    "                                             self.current_step + 1, checkpoint_pretrain_info_tuple,\n",
    "                                             checkpoints_relative_path,\n",
    "                                             checkpoint_dir_name, checkpoint_file_name)\n",
    "\n",
    "                self.current_step += 1\n",
    "\n",
    "                if self.current_step == num_pretrain_iter_steps:\n",
    "                    num_pretrain_iter_steps_reached = True\n",
    "                    break\n",
    "\n",
    "        pretrain_total_time = pretrain_timer.get_total_time()\n",
    "        print(f\"Pretrain BERT Total Time: {format_duration(pretrain_total_time)}\")\n",
    "        print(f\"Pretrain BERT Total Avg MLM Loss: {self.total_mlm_loss / self.current_step:.4f}\")\n",
    "        print(f\"Pretrain BERT Total Num Processed Samples: {self.total_processed_samples}\")\n",
    "        print(\n",
    "            f\"Pretrain BERT Processing Samples Speed: {self.total_processed_samples / pretrain_total_time:.4f} samples/sec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8e6af8-fafa-4919-802e-3128bc1c308c",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-03-20T14:00:36.212434Z",
     "iopub.status.busy": "2025-03-20T14:00:36.212235Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size, max_len = 128, 256\n",
    "pretrain_data_relative_path = \"../../dataset/train/pretrain/pretrain_sentences_test.txt\"\n",
    "# pretrain_data_relative_path = \"../../dataset/train/pretrain/pretrain_sentences_128.txt\"\n",
    "vocab_txt_relative_path = \"../../pretrain_results/vocab.txt\"\n",
    "checkpoints_relative_path = \"../../pretrain_results/checkpoints\"\n",
    "# checkpoint_relative_path = \"../../pretrain_results/checkpoints/checkpoint_step_10/checkpoint_step_10.pth\"\n",
    "\n",
    "pretrain_iter, vocab = load_pretrain_data(pretrain_data_relative_path, vocab_txt_relative_path,\n",
    "                                          batch_size, max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbed5e0ef246689e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-03-20T11:10:37.491761Z"
    },
    "ExecutionIndicator": {
     "show": true
    },
    "jupyter": {
     "is_executing": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "@Project ：NLPNewsClassification\n",
    "@File    ：run_pretrain_bert.py\n",
    "@Author  ：DZY\n",
    "@Date    ：2025/3/17 17:07\n",
    "'''\n",
    "\n",
    "initial_size = 768\n",
    "vocab_size = len(vocab)\n",
    "query_size = initial_size\n",
    "key_size = initial_size\n",
    "value_size = initial_size\n",
    "num_hiddens = initial_size\n",
    "normalized_shape = [initial_size]\n",
    "ffn_num_input = initial_size\n",
    "ffn_num_hiddens = 3072\n",
    "num_heads = 12\n",
    "num_layers = 12\n",
    "mlm_in_features = initial_size\n",
    "mlm_hiddens = initial_size\n",
    "nsp_in_features = initial_size\n",
    "nsp_hiddens = initial_size\n",
    "dropout = 0.1\n",
    "lr = 1e-4\n",
    "num_pretrain_iter_steps = 30\n",
    "\n",
    "net = BERTLM(vocab_size, query_size, key_size, value_size, num_hiddens, normalized_shape, ffn_num_input,\n",
    "             ffn_num_hiddens, num_heads, num_layers, mlm_in_features, mlm_hiddens, nsp_in_features, nsp_hiddens,\n",
    "             dropout, max_len=max_len)\n",
    "\n",
    "devices = try_all_gpus()\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "pretrain_optimizer = torch.optim.AdamW(net.parameters(), lr=lr, weight_decay=0.01)\n",
    "pretrain_scheduler = BERTScheduler(pretrain_optimizer, num_hiddens, warmup_steps=10000)\n",
    "\n",
    "step, total_mlm_loss, total_processed_samples, cum_time_list = load_pretrain_checkpoint(net,\n",
    "                                                                                        pretrain_optimizer,\n",
    "                                                                                        devices[0],\n",
    "                                                                                        checkpoint_relative_path=None)\n",
    "\n",
    "pretrainbert = PretrainBERT(step, total_mlm_loss, total_processed_samples, cum_time_list)\n",
    "pretrainbert.pretrain(pretrain_iter, net, loss, vocab_size, pretrain_optimizer, pretrain_scheduler,\n",
    "                      num_pretrain_iter_steps,\n",
    "                      checkpoints_relative_path, devices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e005dccb14595fb0",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "@Project ：NLPNewsClassification \n",
    "@File    ：fine_tuning_data_create.py\n",
    "@Author  ：DZY\n",
    "@Date    ：2025/3/18 20:19 \n",
    "'''\n",
    "\n",
    "\n",
    "def load_fine_tuning_set_data(fine_tuning_set_relative_path, sequence_length):\n",
    "    script_directory = os.getcwd()\n",
    "    full_path = os.path.join(script_directory, fine_tuning_set_relative_path)\n",
    "    with open(full_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    samples_tokens = []\n",
    "    labels = []\n",
    "    for line in lines:\n",
    "        data = line.split('\\t')\n",
    "        labels.append(int(data[0]))\n",
    "        sample_tokens = data[1].split()\n",
    "        num_sample_tokens = len(sample_tokens)\n",
    "        if num_sample_tokens < sequence_length:\n",
    "            samples_tokens.append(sample_tokens)\n",
    "        else:\n",
    "            samples_tokens.append(sample_tokens[:sequence_length])\n",
    "    return samples_tokens, labels\n",
    "\n",
    "\n",
    "def _create_ft_inputs(samples_tokens, max_len):\n",
    "    \"\"\"\n",
    "    构造所有样本的BERT形式的输入序列\n",
    "\n",
    "    Args:\n",
    "        samples_tokens: 所有样本序列，二维列表，每个子列表是一个样本序列，子列表每个元素是一个词元\n",
    "        max_len: BERT输入序列最大长度\n",
    "\n",
    "    Returns:\n",
    "        所有样本的BERT形式的输入序列及其片段索引\n",
    "        元组列表：[(样本的BERT形式的输入序列,其片段索引),...]\n",
    "\n",
    "    \"\"\"\n",
    "    input_data = []\n",
    "    for sample_tokens in samples_tokens:\n",
    "        # 如果原本输入序列加上<cls>'和'<sep>是否会超过max_len\n",
    "        while (len(sample_tokens) + 2) > max_len:\n",
    "            # 超过就弹出最后一个词\n",
    "            sample_tokens.pop()\n",
    "        tokens, segments = get_tokens_and_segments(sample_tokens)\n",
    "        input_data.append((tokens, segments))\n",
    "    return input_data\n",
    "\n",
    "\n",
    "def _pad_ft_inputs(input_data, max_len, vocab):\n",
    "    all_token_ids, all_segments, valid_lens, = [], [], []\n",
    "    for tokens, segments in input_data:\n",
    "        all_token_ids.append(torch.tensor(vocab[tokens] + [vocab['<pad>']] * (\n",
    "                max_len - len(tokens)), dtype=torch.long))\n",
    "        all_segments.append(torch.tensor(segments + [0] * (\n",
    "                max_len - len(segments)), dtype=torch.long))\n",
    "        # valid_lens不包括'<pad>'的计数\n",
    "        valid_lens.append(torch.tensor(len(tokens), dtype=torch.float32))\n",
    "    return (all_token_ids, all_segments, valid_lens)\n",
    "\n",
    "\n",
    "class FineTuningDataset(Dataset):\n",
    "    def __init__(self, dataset, max_len, vocab):\n",
    "        samples_tokens = dataset[0]\n",
    "        self.labels = torch.tensor(dataset[1])\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "        input_data = _create_ft_inputs(samples_tokens, max_len)\n",
    "        (self.all_token_ids, self.all_segments, self.valid_lens) = _pad_ft_inputs(input_data, max_len, self.vocab)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.all_token_ids[idx], self.all_segments[idx],\n",
    "                self.valid_lens[idx], self.labels[idx])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_token_ids)\n",
    "\n",
    "\n",
    "def create_fine_tuning_iter(batch_size, max_len, sequence_length, vocab, ft_train_set_relative_path,\n",
    "                            ft_validate_set_relative_path, is_train=True):\n",
    "    if is_train:\n",
    "        ft_set_relative_path = ft_train_set_relative_path\n",
    "    else:\n",
    "        ft_set_relative_path = ft_validate_set_relative_path\n",
    "    dataset = load_fine_tuning_set_data(ft_set_relative_path, sequence_length)\n",
    "    ft_dataset = FineTuningDataset(dataset, max_len, vocab)\n",
    "    ft_iter = torch.utils.data.DataLoader(ft_dataset, batch_size, shuffle=is_train, num_workers=0)\n",
    "    return ft_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6ee63caf3b5749",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "@Project ：NLPNewsClassification \n",
    "@File    ：fine_tuning_bert.py\n",
    "@Author  ：DZY\n",
    "@Date    ：2025/3/14 12:38 \n",
    "'''\n",
    "\n",
    "\n",
    "def _load_pretrained_vocab_txt_file(pretrained_vocab_relative_path):\n",
    "    \"\"\"\n",
    "    加载预训练BERT的词表txt文件\n",
    "\n",
    "    Returns:\n",
    "        词元一维列表，每个元素是一个词元\n",
    "\n",
    "    \"\"\"\n",
    "    script_directory = os.getcwd()\n",
    "    pretrained_vocab_txt_file_absolute_path = os.path.join(script_directory, pretrained_vocab_relative_path)\n",
    "    with open(pretrained_vocab_txt_file_absolute_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    return ' '.join(lines).split()\n",
    "\n",
    "\n",
    "def _load_pretrained_vocab(pretrained_vocab_relative_path):\n",
    "    pretrained_vocab_list = _load_pretrained_vocab_txt_file(pretrained_vocab_relative_path)\n",
    "    pretrained_vocab = Vocab()\n",
    "    pretrained_vocab.idx_to_token = pretrained_vocab_list\n",
    "    pretrained_vocab.token_to_idx = {token: idx for idx, token in enumerate(pretrained_vocab.idx_to_token)}\n",
    "    return pretrained_vocab\n",
    "\n",
    "\n",
    "def load_pretrained_model(query_size, key_size, value_size, num_hiddens, normalized_shape, ffn_num_input,\n",
    "                          ffn_num_hiddens, num_heads, num_layers, mlm_in_features, mlm_hiddens, nsp_in_features,\n",
    "                          nsp_hiddens, dropout, max_len, devices, pretrained_vocab_relative_path,\n",
    "                          checkpoint_relative_path):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        query_size:\n",
    "        key_size:\n",
    "        value_size:\n",
    "        num_hiddens:\n",
    "        normalized_shape:\n",
    "        ffn_num_input:\n",
    "        ffn_num_hiddens:\n",
    "        num_heads:\n",
    "        num_layers:\n",
    "        mlm_in_features:\n",
    "        mlm_hiddens:\n",
    "        nsp_in_features:\n",
    "        nsp_hiddens:\n",
    "        dropout:\n",
    "        max_len:\n",
    "        devices:\n",
    "        pretrained_vocab_relative_path:\n",
    "        checkpoint_relative_path:\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    pretrained_vocab = _load_pretrained_vocab(pretrained_vocab_relative_path)\n",
    "    # print(\"pretrained_vocab\",pretrained_vocab.token_to_idx)\n",
    "    pretrained_bert = BERTLM(len(pretrained_vocab), query_size, key_size, value_size, num_hiddens,\n",
    "                                  normalized_shape, ffn_num_input,\n",
    "                                  ffn_num_hiddens, num_heads, num_layers, mlm_in_features, mlm_hiddens, nsp_in_features,\n",
    "                                  nsp_hiddens,\n",
    "                                  dropout, max_len=max_len)\n",
    "    load_pretrained_model_params(pretrained_bert, checkpoint_relative_path, devices[0])\n",
    "    return pretrained_bert, pretrained_vocab\n",
    "\n",
    "\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, pretrained_bert, classifier_num_input, classifier_num_hiddens, classifier_num_output):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.encoder = pretrained_bert.encoder\n",
    "        self.classifier = nn.Sequential(nn.Linear(classifier_num_input, classifier_num_hiddens), nn.ReLU(),\n",
    "                                        nn.Linear(classifier_num_hiddens, classifier_num_output))\n",
    "\n",
    "    def forward(self, tokens, segments, valid_lens):\n",
    "        encoded_X = self.encoder(tokens, segments, valid_lens)\n",
    "        return self.classifier(encoded_X[:, 0, :])\n",
    "\n",
    "\n",
    "# def accuracy(y_hat, y):\n",
    "#     \"\"\"Compute the number of correct predictions.\n",
    "#\n",
    "#     Defined in :numref:`sec_utils`\"\"\"\n",
    "#     if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n",
    "#         y_hat = d2l.argmax(y_hat, axis=1)\n",
    "#     cmp = d2l.astype(y_hat, y.dtype) == y\n",
    "#     return float(d2l.reduce_sum(d2l.astype(cmp, y.dtype)))\n",
    "\n",
    "\n",
    "# todo\n",
    "def get_num_correct_preds(y_hat, y):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        y_hat:\n",
    "        y: 这里y必须是标签列表的下标，比如这里分类是14类，标签是[1,2,3]，分类列表是[1,2,3,...,14]，这里y必须是标签对应分类列表的下标[0,1,2]，这样才能和y_hat对应。\n",
    "        这里简单的做法是把y中每个元素都减一，后面优化代码时，再在数据上做转换\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    preds_max_prob_indices = torch.argmax(y_hat, dim=1)\n",
    "    num_correct_preds = (preds_max_prob_indices == (y - 1)).sum().item()\n",
    "    return num_correct_preds\n",
    "\n",
    "\n",
    "def evaluate_accuracy(net, valid_iter, device=None):\n",
    "    if isinstance(net, nn.Module):\n",
    "        net.eval()\n",
    "        if not device:\n",
    "            device = next(iter(net.parameters())).device\n",
    "    num_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for tokens_X, segments_X, valid_lens_X, labels_Y in valid_iter:\n",
    "            tokens_X = tokens_X.to(device)\n",
    "            segments_X = segments_X.to(device)\n",
    "            valid_lens_X = valid_lens_X.to(device)\n",
    "            labels_Y = labels_Y.to(device)\n",
    "            Y_hat = net(tokens_X, segments_X, valid_lens_X)\n",
    "            num_correct_preds = get_num_correct_preds(Y_hat, labels_Y)\n",
    "            num_samples += tokens_X.shape[0]\n",
    "    return round(num_correct_preds / num_samples, 4)\n",
    "\n",
    "\n",
    "def fine_tuning(ft_train_iter, ft_valid_iter, net, loss, lr, num_epochs, devices, ft_checkpoints_relative_path):\n",
    "    net = nn.DataParallel(net, device_ids=devices).to(devices[0])\n",
    "    ft_optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "    ft_timer = Timer()\n",
    "    current_ft_iter_step = 0\n",
    "    total_ft_train_loss = 0.0\n",
    "    total_ft_train_correct_preds = 0\n",
    "    total_ft_processed_samples = 0\n",
    "    total_ft_valid_acc = 0.0\n",
    "    ft_info_interval = 100\n",
    "\n",
    "    print(\"start fine tuning...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        net.train()\n",
    "        for tokens_X, segments_X, valid_lens_X, labels_Y in ft_train_iter:\n",
    "            tokens_X = tokens_X.to(devices[0])\n",
    "            segments_X = segments_X.to(devices[0])\n",
    "            valid_lens_X = valid_lens_X.to(devices[0])\n",
    "            labels_Y = labels_Y.to(devices[0])\n",
    "\n",
    "            ft_timer.start()\n",
    "            ft_optimizer.zero_grad()\n",
    "            Y_hat = net(tokens_X, segments_X, valid_lens_X)\n",
    "\n",
    "            total_ft_train_correct_preds += get_num_correct_preds(Y_hat, labels_Y)\n",
    "\n",
    "            l = loss(Y_hat, labels_Y)\n",
    "            l.sum().backward()\n",
    "            ft_optimizer.step()\n",
    "            ft_timer.stop()\n",
    "\n",
    "            total_ft_train_loss += l.sum().item()\n",
    "            total_ft_processed_samples += tokens_X.shape[0]\n",
    "\n",
    "            if (current_ft_iter_step + 1) % ft_info_interval == 0:\n",
    "                cum_time_list = ft_timer.get_cumulate_time()\n",
    "                print(\n",
    "                    f\"Iter Steps: {current_ft_iter_step + 2 - ft_info_interval}-{current_ft_iter_step + 1} ---- \"\n",
    "                    f\"Cumulative Avg Loss: {total_ft_train_loss / (current_ft_iter_step + 1):.4f} ---- \"\n",
    "                    f\"Cumulative Correct Train Preds/Cumulative Processed Samples: {total_ft_train_correct_preds}/{total_ft_processed_samples} ---- \"\n",
    "                    f\"Cumulative Avg Train Acc: {total_ft_train_correct_preds / total_ft_processed_samples:.4f} ---- \"\n",
    "                    f\"Cumulative Iter Time: {cum_time_list[current_ft_iter_step]:.4f} sec\")\n",
    "\n",
    "            current_ft_iter_step += 1\n",
    "\n",
    "        total_ft_valid_acc = evaluate_accuracy(net, ft_valid_iter)\n",
    "        print(f\"epoch {epoch} Avg Train Acc: {total_ft_train_correct_preds / total_ft_processed_samples:.4f}\")\n",
    "        print(f\"epoch {epoch} Avg Valid Acc: {total_ft_valid_acc}\")\n",
    "        \n",
    "        checkpoint_dir_name = f\"checkpoint_epoch_{epoch + 1}\"\n",
    "        checkpoint_file_name = f\"checkpoint_epoch_{epoch + 1}.pth\"\n",
    "        save_finetuning_model(net, ft_checkpoints_relative_path, checkpoint_dir_name, checkpoint_file_name)\n",
    "\n",
    "    ft_total_time = ft_timer.get_total_time()\n",
    "    print(\"FT Total Time: \", format_duration(ft_total_time))\n",
    "    print(\"FT Total Steps: \", current_ft_iter_step)\n",
    "    print(f\"FT Avg Train Acc: {total_ft_train_correct_preds / total_ft_processed_samples:.4f}\")\n",
    "    print(\"FT Total Num Processed Samples: \", total_ft_processed_samples)\n",
    "    print(f\"FT Processing Samples Speed: {total_ft_processed_samples / ft_total_time:.4f} samples/sec\")\n",
    "    print(\"FT Avg Valid Acc: \", total_ft_valid_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90d939d281825db",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "@Project ：NLPNewsClassification \n",
    "@File    ：run_fine_tuning_bert.py\n",
    "@Author  ：DZY\n",
    "@Date    ：2025/3/18 21:11 \n",
    "'''\n",
    "\n",
    "\n",
    "max_len = 256\n",
    "initial_size = 768\n",
    "query_size = initial_size\n",
    "key_size = initial_size\n",
    "value_size = initial_size\n",
    "num_hiddens = initial_size\n",
    "normalized_shape = [initial_size]\n",
    "ffn_num_input = initial_size\n",
    "ffn_num_hiddens = 3072\n",
    "num_heads = 12\n",
    "num_layers = 12\n",
    "mlm_in_features = initial_size\n",
    "mlm_hiddens = initial_size\n",
    "nsp_in_features = initial_size\n",
    "nsp_hiddens = initial_size\n",
    "dropout = 0.1\n",
    "devices = try_all_gpus()\n",
    "\n",
    "pretrained_vocab_relative_path = \"../../pretrain_results/vocab.txt\"\n",
    "checkpoints_relative_path = \"../../pretrain_results/checkpoints/checkpoint_step_10/checkpoint_step_10.pth\"\n",
    "ft_train_set_relative_path = \"../../dataset/train/fine_tuning/train/fine_tuning_train_set_test.txt\"\n",
    "ft_validate_set_relative_path = \"../../dataset/train/fine_tuning/validate/fine_tuning_validate_set_test.txt\"\n",
    "ft_checkpoints_relative_path = \"../../fine_tuning_results\"\n",
    "\n",
    "pretrained_bert, pretrained_vocab = load_pretrained_model(query_size, key_size, value_size,\n",
    "                                                                           num_hiddens, normalized_shape, ffn_num_input,\n",
    "                                                                           ffn_num_hiddens, num_heads, num_layers,\n",
    "                                                                           mlm_in_features, mlm_hiddens,\n",
    "                                                                           nsp_in_features, nsp_hiddens,\n",
    "                                                                           dropout, max_len, devices,\n",
    "                                                                           pretrained_vocab_relative_path,\n",
    "                                                                           checkpoints_relative_path)\n",
    "\n",
    "batch_size = 32\n",
    "sequence_length = 256\n",
    "ft_train_iter = create_fine_tuning_iter(batch_size, max_len, sequence_length, pretrained_vocab,\n",
    "                                                                ft_train_set_relative_path,\n",
    "                                                                ft_validate_set_relative_path, is_train=True)\n",
    "ft_valid_iter = create_fine_tuning_iter(batch_size, max_len, sequence_length, pretrained_vocab,\n",
    "                                                                ft_train_set_relative_path,\n",
    "                                                                ft_validate_set_relative_path, is_train=False)\n",
    "\n",
    "# for (tokens_X, segments_X, valid_lens_X, Y) in ft_train_iter:\n",
    "#     print(tokens_X.shape, segments_X.shape, valid_lens_X.shape,Y.shape)\n",
    "#     break\n",
    "\n",
    "\n",
    "lr, num_epochs = 2e-5, 3\n",
    "loss = nn.CrossEntropyLoss(reduction='none')\n",
    "classifier_num_input, classifier_num_hiddens, classifier_num_output = 768, 3072, 14\n",
    "\n",
    "net = BERTClassifier(pretrained_bert, classifier_num_input, classifier_num_hiddens,\n",
    "                                      classifier_num_output)\n",
    "fine_tuning(ft_train_iter, ft_valid_iter, net, loss, lr, num_epochs, devices,\n",
    "                             ft_checkpoints_relative_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
